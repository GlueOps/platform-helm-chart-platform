apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kube-prometheus-stack
  annotations:
    argocd.argoproj.io/sync-wave: "2"
spec:
  destination:
    name: "in-cluster"
    namespace: glueops-core-kube-prometheus-stack
  project: glueops-core
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
    automated:
      prune: true
      selfHeal: true
    retry:
      backoff:
        duration: 10s
        factor: 2
        maxDuration: 3m0s
      limit: 5
  source:
    repoURL: 'https://prometheus-community.github.io/helm-charts'
    targetRevision: 45.17.0
    helm:
      skipCrds: true
      values: |-
        # https://github.com/prometheus-operator/prometheus-operator/issues/2890#issuecomment-583006452
        coreDns:
          enabled: false
        kubeControllerManager:
          enabled: false
        kubeScheduler:
          enabled: false
        kubeProxy:
          enabled: false
        alertmanager:
          alertmanagerSpec:
            image:
              registry: ghcr.io
              repository: glueops/alertmanager
              tag: v0.25.0-glueops
            logLevel: debug
            replicas: 2
            # This externalURL is a hack. Alertmanager will reference back to itself per the code here: https://github.com/prometheus/alertmanager/blob/v0.25.0/template/default.tmpl#L2
            # Example when alert manager renders my hack: 
            # https://grafana.{{ .Values.captain_domain }}/alerting/list
            # - Note: the captain domain does get replaced with the actual captain domain
            externalUrl: https://grafana.{{ .Values.captain_domain }}/alerting/list
        defaultRules:
          disabled:
            KubePodNotReady: true
        prometheus: 
          prometheusSpec:
            replicas: 2
            externalLabels:
              replica: "$(POD_NAME)"
            ruleSelector: {}
            ruleNamespaceSelector: {}
            ruleSelectorNilUsesHelmValues: false
            serviceMonitorSelector: {}
            serviceMonitorNamespaceSelector: {}
            serviceMonitorSelectorNilUsesHelmValues: false
            podMonitorSelector: {}
            podMonitorNamespaceSelector: {}
            podMonitorSelectorNilUsesHelmValues: false
            logLevel: debug
            storageSpec:
              volumeClaimTemplate:
                spec:
                  accessModes:
                    - ReadWriteOnce
                  resources:
                    requests:
                      storage: 50Gi
        grafana:
          adminPassword: "{{ .Values.grafana.admin_password }}"
          ingress:
            enabled: true
            ingressClassName: public-authenticated
            annotations:
              ingress.pomerium.io/allow_any_authenticated_user: 'true'
              ingress.pomerium.io/pass_identity_headers: 'true'
            hosts: ['grafana.{{ .Values.captain_domain }}']
            path: "/"
          additionalDataSources:
          - name: Loki
            type: loki
            url: http://loki-gateway.glueops-core-loki.svc.cluster.local
          grafana.ini:
            server:
              root_url: "https://grafana.{{ .Values.captain_domain }}/"
            auth.generic_oauth:
              enabled: true
              allow_sign_up: true
              name: GitHub SSO
              client_id: grafana
              client_secret: "{{ .Values.dex.grafana.client_secret }}"
              scopes: "openid profile email groups"
              auth_url: https://dex.{{ .Values.captain_domain }}/auth
              token_url: https://dex.{{ .Values.captain_domain }}/token
              api_url: https://dex.{{ .Values.captain_domain }}/userinfo
              ssl_verify: false
              role_attribute_path: contains(groups[*], '{{ .Values.grafana.github_admin_org_name }}:{{ .Values.grafana.github_admin_team_name }}') && 'Admin' || 'Viewer'
    chart: kube-prometheus-stack
