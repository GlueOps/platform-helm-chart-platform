apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kube-prometheus-stack
  annotations:
    argocd.argoproj.io/sync-wave: "2"
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  destination:
    name: "in-cluster"
    namespace: glueops-core-kube-prometheus-stack
  project: glueops-core
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
    automated:
      prune: true
      selfHeal: true
    retry:
      backoff:
        duration: 10s
        factor: 2
        maxDuration: 3m0s
      limit: 5
  source:
    repoURL: 'https://prometheus-community.github.io/helm-charts'
    chart: kube-prometheus-stack
    targetRevision: 74.2.2
    helm:
      skipCrds: true
      values: |-
        # https://github.com/prometheus-operator/prometheus-operator/issues/2890#issuecomment-583006452
        coreDns:
          enabled: false
        # https://github.com/prometheus-community/helm-charts/issues/1033
        fullnameOverride: "kps"
        kubeControllerManager:
          enabled: false
        kubeScheduler:
          enabled: false
        kubeProxy:
          enabled: false
        kube-state-metrics:
          {{- toYaml .Values.glueops_node_and_tolerations | nindent 10 }}
        alertmanager:
          alertmanagerSpec:
            {{- toYaml .Values.glueops_node_and_tolerations | nindent 12 }}
            logLevel: debug
            replicas: 2
            image:
              registry: {{ .Values.base_registries.quay_io }}
            # This externalURL is a hack. Alertmanager will reference back to itself per the code here: https://github.com/prometheus/alertmanager/blob/v0.25.0/template/default.tmpl#L2
            # Example when alert manager renders my hack: 
            # https://grafana.{{ .Values.captain_domain }}/alerting/list
            # - Note: the captain domain does get replaced with the actual captain domain
            externalUrl: https://grafana.{{ .Values.captain_domain }}/alerting/list
        defaultRules:
          disabled:
            KubePodNotReady: true
        prometheusOperator:
          image:
            registry: {{ .Values.base_registries.quay_io }}
          prometheusConfigReloader:
            image:
              registry: {{ .Values.base_registries.quay_io }}
          thanosImage:
            registry: {{ .Values.base_registries.quay_io }}
          hostNetwork: {{ .Values.host_network.enabled }}
          tls:
            internalPort: {{ .Values.host_network.kube_pometheus_stack.prometheusOperator.tls.internal_port }}
          admissionWebhooks:
            deployment:
              image:
                registry: {{ .Values.base_registries.quay_io }}
              tls:
                internalPort: {{ .Values.host_network.kube_pometheus_stack.prometheusOperator.admissionWebhooks.deployment.tls.internal_port }}
              hostNetwork: {{ .Values.host_network.enabled }}
            patch:
              image:
                registry: {{ .Values.base_registries.registry_k8s_io }}
              {{- toYaml .Values.glueops_node_and_tolerations | nindent 14 }}
          {{- toYaml .Values.glueops_node_and_tolerations | nindent 10 }}
        {{- if .Values.kube_etcd.enabled }}
        kubeEtcd:
          serviceMonitor:
            caFile: {{ .Values.kube_etcd.serviceMonitor.caFile }}
            certFile: {{ .Values.kube_etcd.serviceMonitor.certFile }}
            keyFile: {{ .Values.kube_etcd.serviceMonitor.keyFile }}
            scheme: https
          service:
            port: 2379
            targetPort: 2379
        {{- end }}
        prometheus: 
          enabled: true
          prometheusSpec:
            {{- toYaml .Values.glueops_node_and_tolerations | nindent 12 }}
            replicas: 2
            image:
              registry: {{ .Values.base_registries.quay_io }}
            ruleSelector: {}
            ruleNamespaceSelector: {}
            ruleSelectorNilUsesHelmValues: false
            serviceMonitorSelector: {}
            serviceMonitorNamespaceSelector: {}
            serviceMonitorSelectorNilUsesHelmValues: false
            podMonitorSelector: {}
            podMonitorNamespaceSelector: {}
            podMonitorSelectorNilUsesHelmValues: false
            logLevel: debug
            storageSpec:
              volumeClaimTemplate:
                spec:
                  accessModes:
                    - ReadWriteOnce
                  resources:
                    requests:
                      storage: {{ .Values.prometheus.volume_claim_storage_request}}Gi
            hostNetwork: {{ .Values.host_network.enabled }}
            {{- if .Values.kube_etcd.enabled }}
            secrets:
              - etcd-client
            {{- end }}
        grafana:
          {{- toYaml .Values.glueops_node_and_tolerations | nindent 10 }}
          adminPassword: "{{ .Values.grafana.admin_password }}"
          image:
            registry: {{ .Values.container_images.app_kube_prometheus_stack.grafana.image.registry }}
            repository: {{ .Values.container_images.app_kube_prometheus_stack.grafana.image.repository }}
            tag: {{ .Values.container_images.app_kube_prometheus_stack.grafana.image.tag }}
          #https://stackoverflow.com/questions/77912105/upgrading-kube-prometheus-stack-via-helm-to-chart-v56-2-1-fails-on-grafana-with
          assertNoLeakedSecrets: false
          ingress:
            enabled: true
            ingressClassName: glueops-platform
            annotations:
              nginx.ingress.kubernetes.io/auth-signin: "https://oauth2.{{ .Values.captain_domain }}/oauth2/start?rd=https://$host$request_uri"
              nginx.ingress.kubernetes.io/auth-url: "https://oauth2.{{ .Values.captain_domain }}/oauth2/auth"
              nginx.ingress.kubernetes.io/auth-response-headers: "authorization, x-auth-request-user, x-auth-request-email, x_auth_request_access_token"
            hosts: ['grafana.{{ .Values.captain_domain }}']
            path: "/"
          additionalDataSources:
          - name: Loki
            type: loki
            url: http://loki-gateway.glueops-core-loki.svc.cluster.local
          grafana.ini:
            users:
              viewers_can_edit: true
            server:
              root_url: "https://grafana.{{ .Values.captain_domain }}/"
            auth.generic_oauth:
              enabled: true
              allow_sign_up: true
              name: GitHub SSO
              client_id: grafana
              client_secret: "{{ .Values.dex.grafana.client_secret }}"
              scopes: "openid profile email groups"
              auth_url: https://dex.{{ .Values.captain_domain }}/auth
              token_url: https://dex.{{ .Values.captain_domain }}/token
              api_url: https://dex.{{ .Values.captain_domain }}/userinfo
              ssl_verify: false
              role_attribute_path: contains(groups[*], '{{ .Values.grafana.github_admin_org_name }}:{{ .Values.grafana.github_admin_team_name }}') && 'Admin' || 'Viewer'
        thanosRuler:
          thanosRulerSpec:
            image:
              registry: {{ .Values.base_registries.quay_io }}