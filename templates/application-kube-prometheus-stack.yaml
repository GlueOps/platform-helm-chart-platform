apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kube-prometheus-stack
  annotations:
    argocd.argoproj.io/sync-wave: "2"
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  destination:
    name: "in-cluster"
    namespace: glueops-core-kube-prometheus-stack
  project: glueops-core
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
    automated:
      prune: true
      selfHeal: true
    retry:
      backoff:
        duration: 10s
        factor: 2
        maxDuration: 3m0s
      limit: 5
  source:
    repoURL: 'https://prometheus-community.github.io/helm-charts'
    chart: kube-prometheus-stack
    targetRevision: 59.1.0
    helm:
      skipCrds: true
      values: |-
        # https://github.com/prometheus-operator/prometheus-operator/issues/2890#issuecomment-583006452
        coreDns:
          enabled: false
        # https://github.com/prometheus-community/helm-charts/issues/1033
        fullnameOverride: "kps"
        kubeControllerManager:
          enabled: {{ .Values.prometheus.kube_controller_manager }}
        kubeScheduler:
          enabled: {{ .Values.prometheus.kube_scheduler }}
        kubeProxy:
          enabled: {{ .Values.prometheus.kube_proxy }}
        kubeApiServer:
          enabled: {{ .Values.prometheus.kube_api_server }}
        kubelet:
          enabled: {{ .Values.prometheus.kubelet }}
        serviceMonitor:
          enabled: {{ .Values.prometheus.service_monitor }}
        kubeStateMetrics:
          enabled: {{ .Values.prometheus.kube_state_metric }}    
        nodeExporter:
          enabled: {{ .Values.prometheus.node_exporter }}    
        alertmanager:
          alertmanagerSpec:
            {{- toYaml .Values.glueops_node_and_tolerations | nindent 12 }}
            logLevel: {{ .Values.prometheus.alertmanager_log_level }}
            replicas: 2
            # This externalURL is a hack. Alertmanager will reference back to itself per the code here: https://github.com/prometheus/alertmanager/blob/v0.25.0/template/default.tmpl#L2
            # Example when alert manager renders my hack: 
            # https://grafana.{{ .Values.captain_domain }}/alerting/list
            # - Note: the captain domain does get replaced with the actual captain domain
            externalUrl: https://grafana.{{ .Values.captain_domain }}/alerting/list
        defaultRules:
          disabled:
            KubePodNotReady: true
        prometheusOperator:
          hostNetwork: {{ .Values.host_network.enabled }}
          tls:
            internalPort: {{ .Values.host_network.kube_pometheus_stack.prometheusOperator.tls.internal_port }}
          admissionWebhooks:
            deployment:
              tls:
                internalPort: {{ .Values.host_network.kube_pometheus_stack.prometheusOperator.admissionWebhooks.deployment.tls.internal_port }}
              hostNetwork: {{ .Values.host_network.enabled }}
            patch:
              {{- toYaml .Values.glueops_node_and_tolerations | nindent 14 }}
          {{- toYaml .Values.glueops_node_and_tolerations | nindent 10 }}
        prometheus: 
          enabled: true
          thanosService:
            enabled: true
          thanosServiceMonitor:
            enabled: true
          prometheusSpec:
            {{- toYaml .Values.glueops_node_and_tolerations | nindent 12 }}
            replicas: 2
            replicaExternalLabelName: "__replica__"
            externalLabels: {cluster: "test"}
            ruleSelector: {}
            ruleNamespaceSelector: {}
            ruleSelectorNilUsesHelmValues: false
            serviceMonitorSelector: {}
            serviceMonitorNamespaceSelector: {}
            serviceMonitorSelectorNilUsesHelmValues: false
            podMonitorSelector: {}
            podMonitorNamespaceSelector: {}
            podMonitorSelectorNilUsesHelmValues: false
            enableRemoteWriteReceiver: true
            disableCompaction: false
            retention: {{ .Values.prometheus.retention }}
            retentionSize: {{ .Values.prometheus.retentionSize }}
            thanos:
              baseImage: quay.io/thanos/thanos
              version: v0.36.1
              objectStorageConfig:
                secret: 
                {{- toYaml .Values.thanos.storage | nindent 18 }}
                  
            enableFeatures:
               - remote-write-receiver
            storageSpec:
              volumeClaimTemplate:
                spec:
                  accessModes:
                    - ReadWriteOnce
                  resources:
                    requests:
                      storage: {{ .Values.prometheus.volume_claim_storage_request}}Gi
            hostNetwork: {{ .Values.host_network.enabled }}
        grafana:
          {{- toYaml .Values.glueops_node_and_tolerations | nindent 10 }}
          adminPassword: "{{ .Values.grafana.admin_password }}"
          image:
            registry: {{ .Values.container_images.app_kube_prometheus_stack.grafana.image.registry }}
            repository: {{ .Values.container_images.app_kube_prometheus_stack.grafana.image.repository }}
            tag: {{ .Values.container_images.app_kube_prometheus_stack.grafana.image.tag }}
          #https://stackoverflow.com/questions/77912105/upgrading-kube-prometheus-stack-via-helm-to-chart-v56-2-1-fails-on-grafana-with
          assertNoLeakedSecrets: false
          plugins:
           - yesoreyeram-infinity-datasource
          ingress:
            enabled: true
            ingressClassName: public-authenticated
            annotations:
              ingress.pomerium.io/allow_any_authenticated_user: 'true'
              ingress.pomerium.io/pass_identity_headers: 'true'
              ingress.pomerium.io/allow_websockets: 'true'
              ingress.pomerium.io/idle_timeout: 0s
              ingress.pomerium.io/preserve_host_header: 'true'
            hosts: ['grafana.{{ .Values.captain_domain }}']
            path: "/"
          sidecar:
            datasources:
              enabled: true
              defaultDatasourceEnabled: true
              isDefaultDatasource: true
              url: http://kube-thanos-stack-query.glueops-core-kube-prometheus-stack:9090/
              name: Prometheus
              uid: prometheus    
          additionalDataSources:
          - name: Loki
            type: loki
            url: http://loki-gateway.glueops-core-loki.svc.cluster.local
          grafana.ini:
            users:
              viewers_can_edit: true
            server:
              root_url: "https://grafana.{{ .Values.captain_domain }}/"
            auth.generic_oauth:
              enabled: true
              allow_sign_up: true
              name: GitHub SSO
              client_id: grafana
              client_secret: "{{ .Values.dex.grafana.client_secret }}"
              scopes: "openid profile email groups"
              auth_url: https://dex.{{ .Values.captain_domain }}/auth
              token_url: https://dex.{{ .Values.captain_domain }}/token
              api_url: https://dex.{{ .Values.captain_domain }}/userinfo
              ssl_verify: false
              role_attribute_path: contains(groups[*], '{{ .Values.grafana.github_admin_org_name }}:{{ .Values.grafana.github_admin_team_name }}') && 'Admin' || 'Viewer'
