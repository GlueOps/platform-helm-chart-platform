apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: glueops-monitoring
  annotations:
    argocd.argoproj.io/sync-wave: "10"
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  destination:
    name: "in-cluster"
    namespace: glueops-monitoring
  project: glueops-core
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
    automated:
      prune: true
      selfHeal: true
    retry:
      backoff:
        duration: 30s
        factor: 2
        maxDuration: 5m0s
      limit: 5
  source:
    repoURL: 'https://github.com/GlueOps/k8s-monitoring-helm.git'
    path: charts/glueops-monitoring
    targetRevision: main
    helm:
      skipCrds: true
      values: |-
        kube-prometheus-stack:
          # https://github.com/prometheus-operator/prometheus-operator/issues/2890#issuecomment-583006452
          coreDns:
            enabled: false
          # https://github.com/prometheus-community/helm-charts/issues/1033
          fullnameOverride: "kps"
          kubeControllerManager:
            enabled: {{ .Values.prometheus.kube_controller_manager }}
          kubeScheduler:
            enabled: {{ .Values.prometheus.kube_scheduler }}
          kubeProxy:
            enabled: {{ .Values.prometheus.kube_proxy }}
          kubeApiServer:
            enabled: {{ .Values.prometheus.kube_api_server }}
          kubelet:
            enabled: {{ .Values.prometheus.kubelet }}
          serviceMonitor:
            enabled: {{ .Values.prometheus.service_monitor }}
          kubeStateMetrics:
            enabled: {{ .Values.prometheus.kube_state_metric }}    
          nodeExporter:
            enabled: {{ .Values.prometheus.node_exporter }}    
          alertmanager:
            alertmanagerSpec:
              {{- toYaml .Values.glueops_node_and_tolerations | nindent 12 }}
              logLevel: {{ .Values.prometheus.alertmanager_log_level }}
              replicas: 2
              # This externalURL is a hack. Alertmanager will reference back to itself per the code here: https://github.com/prometheus/alertmanager/blob/v0.25.0/template/default.tmpl#L2
              # Example when alert manager renders my hack: 
              # https://grafana.{{ .Values.captain_domain }}/alerting/list
              # - Note: the captain domain does get replaced with the actual captain domain
              externalUrl: https://grafana.{{ .Values.captain_domain }}/alerting/list
          defaultRules:
            disabled:
              KubePodNotReady: true
          kube-state-metrics:
            metricLabelsAllowlist: 
              - pods=[*]  
          prometheusOperator:
            hostNetwork: {{ .Values.host_network.enabled }}
            tls:
              internalPort: {{ .Values.host_network.kube_pometheus_stack.prometheusOperator.tls.internal_port }}
            admissionWebhooks:
              deployment:
                tls:
                  internalPort: {{ .Values.host_network.kube_pometheus_stack.prometheusOperator.admissionWebhooks.deployment.tls.internal_port }}
                hostNetwork: {{ .Values.host_network.enabled }}
              patch:
                {{- toYaml .Values.glueops_node_and_tolerations | nindent 14 }}
            {{- toYaml .Values.glueops_node_and_tolerations | nindent 10 }}
          prometheus: 
            enabled: true
            thanosService:
              enabled: true
            thanosServiceMonitor:
              enabled: true
            prometheusSpec:
              {{- toYaml .Values.glueops_node_and_tolerations | nindent 14 }}
              replicas: 2
              replicaExternalLabelName: "__replica__"
              resources:
                requests:
                  memory: 2Gi  # Minimum request
                limits:
                  memory: 4Gi 
              externalLabels:
                captain_domain: {{ .Values.captain_domain }}
              ruleSelector: {}
              ruleNamespaceSelector: {}
              ruleSelectorNilUsesHelmValues: false
              serviceMonitorSelector: {}
              serviceMonitorNamespaceSelector: {}
              serviceMonitorSelectorNilUsesHelmValues: false
              podMonitorSelector: {}
              podMonitorNamespaceSelector: {}
              podMonitorSelectorNilUsesHelmValues: false
              enableRemoteWriteReceiver: true
              disableCompaction: false
              retention: {{ .Values.prometheus.retention }}
              retentionSize: {{ .Values.prometheus.retentionSize }}
              thanos:
                baseImage: quay.io/thanos/thanos
                version: v0.38.0
                objectStorageConfig:
                  secret: 
                  {{- toYaml .Values.thanos.storage | nindent 18 }}
                    
              enableFeatures:
                - remote-write-receiver
              storageSpec:
                volumeClaimTemplate:
                  spec:
                    accessModes:
                      - ReadWriteOnce
                    resources:
                      requests:
                        storage: {{ .Values.prometheus.volume_claim_storage_request}}Gi
              hostNetwork: {{ .Values.host_network.enabled }}
          grafana:
            enabled: false
            {{- toYaml .Values.glueops_node_and_tolerations | nindent 10 }}
            adminPassword: "{{ .Values.grafana.admin_password }}"
            image:
              registry: {{ .Values.container_images.app_kube_prometheus_stack.grafana.image.registry }}
              repository: {{ .Values.container_images.app_kube_prometheus_stack.grafana.image.repository }}
              tag: {{ .Values.container_images.app_kube_prometheus_stack.grafana.image.tag }}
            #https://stackoverflow.com/questions/77912105/upgrading-kube-prometheus-stack-via-helm-to-chart-v56-2-1-fails-on-grafana-with
            assertNoLeakedSecrets: false
            plugins:
            - yesoreyeram-infinity-datasource
            ingress:
              enabled: true
              ingressClassName: public-authenticated
              annotations:
                ingress.pomerium.io/allow_any_authenticated_user: 'true'
                ingress.pomerium.io/pass_identity_headers: 'true'
                ingress.pomerium.io/allow_websockets: 'true'
                ingress.pomerium.io/idle_timeout: 0s
                ingress.pomerium.io/preserve_host_header: 'true'
              hosts: ['grafana.{{ .Values.captain_domain }}']
              path: "/"
            sidecar:
              datasources:
                enabled: true
                defaultDatasourceEnabled: true
                isDefaultDatasource: true
                url: http://glueops-monitoring-thanos-query.glueops-monitoring:9090
                name: Prometheus
                uid: prometheus  
            additionalDataSources:
            - name: Loki
              type: loki
              url: http://loki-gateway.glueops-monitoring-loki.svc.cluster.local
              jsonData:
                httpHeaderName1: "X-Scope-OrgID"
              secureJsonData:
                httpHeaderValue1: "{{ .Values.captain_domain }}"
            - name: Tempo
              type: tempo
              uid: de7lydl3hl9fkd
              url: http://grafana-tempo-query-frontend.glueops-monitoring-tempo:3100
              jsonData:
                httpHeaderName1: "X-Scope-OrgID"
              secureJsonData:
                httpHeaderValue1: "{{ .Values.captain_domain }}"
            grafana.ini:
              users:
                viewers_can_edit: true
              server:
                root_url: "https://grafana.{{ .Values.captain_domain }}/"
              auth.generic_oauth:
                enabled: true
                allow_sign_up: true
                name: GitHub SSO
                client_id: grafana
                client_secret: "{{ .Values.dex.grafana.client_secret }}"
                scopes: "openid profile email groups"
                auth_url: https://dex.{{ .Values.captain_domain }}/auth
                token_url: https://dex.{{ .Values.captain_domain }}/token
                api_url: https://dex.{{ .Values.captain_domain }}/userinfo
                ssl_verify: false
                org_attribute_path:   
                role_attribute_path: contains(groups[*], '{{ .Values.grafana.github_admin_org_name }}:{{ .Values.grafana.github_admin_team_name }}') && 'Admin' || 'Viewer'
        thanos:
          thanos:
            objstoreConfig:
            {{- toYaml .Values.thanos.storage | nindent 12 }}
            global:
              security:
                allowInsecureImages: true
            image:
              registry: ghcr.io
              repository: glueops/mirror/bitnami/thanos
            metrics:
              enabled: true
              serviceMonitor:
                enabled: true
            query:
              replicaLabel: ["__replica__"]
              resources: 
                requests:
                  cpu: 2
                  memory: 1024Mi
                limits:
                  cpu: 3
                  memory: 2024Mi
              dnsDiscovery:
                enabled: true
                logLevel: debug
                sidecarsService: "kps-thanos-discovery"
                sidecarsNamespace: "glueops-monitoring-kube-prometheus-stack"
            bucketweb:
              enabled: true
            compactor:
              enabled: true
              retentionResolutionRaw: 15d
              retentionResolution5m: 30d
              retentionResolution1h: 10y
              resources:
                requests:
                  cpu: 2
                  memory: 512Mi
                limits:
                  cpu: 3
                  memory: 2024Mi
            storegateway:
              enabled: true
              useEndpointGroup: true
        loki:
          loki:
            auth_enabled: true
            multi_tenant_mode: true
            pattern_ingester:
              enabled: true
            frontend:
              log_queries_longer_than: 5s
              compress_responses: true
              #cache_results: true
            runtimeConfig:
              overrides:
                {{ .Values.captain_domain }}:
                  ingestion_rate_mb: 24
                  ingestion_burst_size_mb: 36
                  max_streams_per_user: 100000
                  max_chunks_per_query: 100000
            limits_config:
              ingestion_rate_mb: 24      # Prevent high ingestion rates
              ingestion_burst_size_mb: 36
              max_query_parallelism: 4    # Limit concurrent queries
              max_query_series: 10000     # Prevent large query memory spikes
              max_streams_per_user: 1000  # Restrict high memory-consuming logs
            structuredConfig:
              ingester:
                  max_chunk_age: 2m         # Flush chunks quickly to S3
                  chunk_idle_period: 1m      # Close idle chunks faster
                  chunk_retain_period: 30s   # Keep chunks in memory for minimal time
                  
            storage:
            {{- toYaml .Values.loki.storage | nindent 14 }}
            schemaConfig:
              configs:
                - from: 2024-04-01
                  store: tsdb
                  object_store: s3
                  schema: v13
                  index:
                    prefix: loki_index_
                    period: 24h
          
            ingester:
                chunk_encoding: snappy
            tracking:
                enabled: true
            querier:
                max_concurrent: 4
          
          deploymentMode: SimpleScalable
          backend:
            replicas: 3
          read:
            replicas: 3
            legacyReadTarget: false
          write:
            replicas: 3
          minio:
            enabled: false
          singleBinary:
            replicas: 0  
          ingester:
            replicas: 0
          querier:
            replicas: 0
          queryFrontend:
            replicas: 0
          queryScheduler:
              replicas: 0
          distributor:
            replicas: 0
          compactor:
            replicas: 0
          indexGateway:
            replicas: 0
          bloomCompactor:
            replicas: 0
          bloomGateway:
            replicas: 0  
          ruler:
            extraVolumes:
              - name: fake
                emptyDir: {}
            extraVolumeMounts:
              - name: fake
                mountPath: /tmp/loki/rules/fake 
        opentelemetry-operator:
          replicaCount: {{ .Values.otel.replicas }}
          nameOverride: {{ .Values.otel.name_override }}
          manager:
            {{- toYaml .Values.otel.manager | nindent 12 }}
        grafana-dev:
          fullnameOverride: "grafana-dev"
          adminPassword: "{{ .Values.grafana.admin_password }}"
          sidecar:
            dashboards:
              enabled: true
              searchNamespace: ALL
              label: grafana_developer_dashboard
              defaultDatasourceEnabled: false
              isDefaultDatasource: true
              url: http://kps-prometheus.glueops-monitoring:9090/
              name: Prometheus
              uid: prometheus

          datasources:
            datasources.yaml:
              apiVersion: 1
              datasources:
                - name: Prometheus
                  type: prometheus
                  uid: prometheus
                  url:  http://kps-prometheus.glueops-monitoring:9090/
                  isDefault: true
                - name: Loki
                  type: loki
                  url: http://glueops-monitoring-loki-gateway.glueops-monitoring
                  jsonData:
                    httpHeaderName1: "X-Scope-OrgID"
                  secureJsonData:
                    httpHeaderValue1: "{{ .Values.captain_domain }}"
                - name: Tempo
                  type: tempo
                  uid: de7lydl3hl9fkd
                  url: http://glueops-monitoring-tempo-query-frontend.glueops-monitoring:3100
                  jsonData:
                    httpHeaderName1: "X-Scope-OrgID"
                    nodeGraph:
                      enabled: true
                    serviceMap:
                      datasourceUid: prometheus
                    tracesToMetrics:
                      datasourceUid: prometheus  
                  secureJsonData:
                    httpHeaderValue1: "{{ .Values.captain_domain }}"    
            
          image:
            registry: {{ .Values.container_images.app_kube_prometheus_stack.grafana.image.registry }}
            repository: {{ .Values.container_images.app_kube_prometheus_stack.grafana.image.repository }}
            tag: {{ .Values.container_images.app_kube_prometheus_stack.grafana.image.tag }}
          plugins:
            - yesoreyeram-infinity-datasource
          assertNoLeakedSecrets: false
          ingress:
            enabled: true
            ingressClassName: public-authenticated
            annotations:
              ingress.pomerium.io/allow_any_authenticated_user: 'true'
              ingress.pomerium.io/pass_identity_headers: 'true'
              ingress.pomerium.io/allow_websockets: 'true'
              ingress.pomerium.io/idle_timeout: 0s
              ingress.pomerium.io/preserve_host_header: 'true'
            hosts:
              - grafana.{{ .Values.captain_domain }}
            path: "/"
          additionalDataSources:
          - name: Loki
            type: loki
            url: http://glueops-monitoring-loki-gateway.glueops-monitoring
            jsonData:
              httpHeaderName1: "X-Scope-OrgID"
            secureJsonData:
              httpHeaderValue1: "{{ .Values.captain_domain }}"
          - name: Tempo
            type: tempo
            uid: de7lydl3hl9fkd
            url: http://glueops-monitoring-tempo-query-frontend.glueops-monitoring:3100
            jsonData:
              httpHeaderName1: "X-Scope-OrgID"
              nodeGraph:
                enabled: true
              serviceMap:
                datasourceUid: prometheus
              tracesToMetrics:
                datasourceUid: prometheus 
            secureJsonData:
              httpHeaderValue1: "{{ .Values.captain_domain }}"
            
          grafana.ini:
            users:
              viewers_can_edit: true
            server:
              root_url: "https://grafana.{{ .Values.captain_domain }}/"
            auth.generic_oauth:
              enabled: true
              allow_sign_up: true
              name: GitHub SSO
              client_id: grafana
              client_secret: "{{ .Values.dex.grafana.client_secret }}"
              scopes: "openid profile email groups"
              auth_url: https://dex.{{ .Values.captain_domain }}/auth
              token_url: https://dex.{{ .Values.captain_domain }}/token
              api_url: https://dex.{{ .Values.captain_domain }}/userinfo
              ssl_verify: false
              org_attribute_path:   
              role_attribute_path: contains(groups[*], '{{ .Values.grafana.github_admin_org_name }}:{{ .Values.grafana.github_admin_team_name }}') && 'Admin' || 'Viewer'
        tempo-distributed:
          tempo:
            image:
              tag: 2.7.2
          compactor:
            config:
              compaction:
                block_retention: {{ .Values.tempo.compaction_block_retention}}
          multitenancyEnabled: true
          gateway:
            enabled: true
          metaMonitoring:
            serviceMonitor:
              enabled: true  
          ingester:  
            config:
              flush_check_period: 10s
              flush_all_on_shutdown: true
              complete_block_timeout: 1m
              max_block_bytes: 524288000
              trace_idle_period: 10s
          server:
              grpc_server_max_recv_msg_size: 81194305
              grpc_server_max_send_msg_size: 81194305
          storage:
            trace: 
             {{- toYaml .Values.tempo.storage | nindent 14 }}
          traces:
            otlp:
              http:
                enabled: true
              grpc:
                enabled: true
                max_recv_msg_size_mib: 8194305

